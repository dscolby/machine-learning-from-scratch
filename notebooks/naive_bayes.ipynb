{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f4bf12a",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "#### Author: Darren Colby\n",
    "#### Course: COSC 74\n",
    "#### Date: March 10th, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734a9eaf",
   "metadata": {},
   "source": [
    "## Setting up the notebook for subsequent questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94f70182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01b98a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "naive = np.loadtxt(\"hw4_naive.csv\", delimiter = \",\", skiprows=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f8eefe",
   "metadata": {},
   "source": [
    "## Don't be so naive!\n",
    "These questions are all related to implementing and using the Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3446fb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify features and labels\n",
    "x = naive[:, :-1]\n",
    "y = naive[:, -1]\n",
    "\n",
    "# Create train-test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb43676",
   "metadata": {},
   "source": [
    "### Fitting the model\n",
    "The first step here is fitting our model. Unlike other supervised learning methods that rely on gradient descent to learn some weights that minimize a cost function, Naive Bayes is a probabilistic model. This means the only thing we need to do in this step is calculate the priors for each class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "599a752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(y_train):\n",
    "    \"\"\"Calculates the prior for each class to use in a Bayesian framework\n",
    "       ------------------------------------------------------------------\n",
    "       \n",
    "       Parameters:\n",
    "           y_train: a 1D array of classes to predict from the training data\n",
    "        \n",
    "       Returns a dictionary where keys are classes and values are their priors\"\"\"\n",
    "    # The unique classes in the dataset\n",
    "    labels = np.unique(y_train)\n",
    "    \n",
    "    # Each class is a key and its prior is the value\n",
    "    priors_dict = {label:np.log(y_train[y_train == label].size / y_train.size) for label in labels}\n",
    "    \n",
    "    return priors_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef94ac0",
   "metadata": {},
   "source": [
    "### Calculating the likelihood\n",
    "In Naive Bayes we need to calculate the likelihood that the value of each feature in each observation of our test dataset came from the same feature in the training data set conditional on its class label. In real life this involves multiplying the probabilities that each value in the test data set came from each feature conditional on each class label in the traiing data set. However, to avoid floating point precision errors, we add the log of each of these probabilities, which gives us the same predicted classes. Since calculating these probabilities depends on the probability distribution of the features in our traiing data, we need to use the probability density or probability mass function for each probability distribution we assume our data was generated from. For our purposes, we assume features are either multinomially or Gaussian distributed and therefore define functions to calculate the Gaussian and multinomial log-likelihoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa8149be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multinomial_log_likelihood(row, subsetted_data, smoothing, eps):\n",
    "    \"\"\"Calculates and sums the log of posteriors for each feature in a row of data in a numpy array from the Gausssian PDF\n",
    "       -------------------------------------------------------------------------------------------------------------------\n",
    "       \n",
    "       Parameters:\n",
    "           row: the row of data to calculate the multinomial log-likelihood for\n",
    "           subsetted_data: a 2D numpy array of data subsetted to a class of interest\n",
    "           smoothing: a smoothing parameter for Laplace smoothing\n",
    "           eps: a constant to add to probabilities before taking their log to avoid dividing by zero\n",
    "           \n",
    "       Returns the multinomical log-likelihood for the given row of data\"\"\"\n",
    "    \n",
    "    # The sum of the log probabilities for each feature\n",
    "    total_log_prob = 0\n",
    "    \n",
    "    # Loop through each feature in each data point\n",
    "    for num in row:\n",
    "       \n",
    "        # Get the index of the array, which corresponds to the column to use for the multinomial PDF\n",
    "        idx = np.where(row == num)\n",
    "        \n",
    "        # Find the number of observations with the same value and all values in the training data subsetted by class\n",
    "        same_obs = np.where(subsetted_data[:, idx] == num)[0].size + smoothing\n",
    "        number_obs = subsetted_data[:, idx].size + subsetted_data.shape[1]\n",
    "        \n",
    "        # Update the total log likelihood\n",
    "        total_log_prob += np.log((same_obs / number_obs) + eps)\n",
    "        \n",
    "    return total_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bed7db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_log_likelihood(row, subsetted_data, eps):\n",
    "    \"\"\"Calculates and sums the log of posteriors for each feature in a row of data in a numpy array from the Gausssian PDF\n",
    "       ------------------------------------------------------------------------------------------------------------------\n",
    "       \n",
    "       Parameters:\n",
    "           row: the row of data to calculate the gaussian log-likelihood for\n",
    "           subsetted_data: a 2D numpy array of data subsetted to a class of interest\n",
    "           eps: a constant to add to probabilities before taking their log to avoid dividing by zero\n",
    "           \n",
    "        Returns the Gaussian log-likelihood for the given row of data\"\"\"\n",
    "    \n",
    "    # The sum of log probabilities for each feature\n",
    "    total_log_prob = 0\n",
    "    \n",
    "    # Loop through each feature of the data point\n",
    "    for num in row:\n",
    "        \n",
    "        # Get the index in the array. This corresponds to the column to get the mean and standard deviation of.\n",
    "        idx = np.where(row == num)\n",
    "        \n",
    "        # Parameters of the normal distribution in the column corresponding to the index \n",
    "        mean = np.mean(subsetted_data[:, idx])\n",
    "        std_dev = np.std(subsetted_data[:, idx])\n",
    "        \n",
    "        # Add the log of the probability from the Gaussian PDF\n",
    "        total_log_prob += np.log(((1.0 / np.sqrt(2.0 * np.pi * std_dev**2)) * np.exp(-((num - mean)**2) / \\\n",
    "                                                                                     (2 * (std_dev**2)))) + eps)\n",
    "    \n",
    "    return total_log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad040a1",
   "metadata": {},
   "source": [
    "### Making predictions\n",
    "To make predictions, we need to go through each data point and calculate the log-likelihood that each of its feature values came from the feature in the training data. We do this with each class label by subsetting the data to observations whose class label is the class we are iterating through and summing them up. Since we are using a Bayesian framework, we also add the log of the prior to each observation for each class. The code below does this, however, it only uses one for loop because it takes advantage of NumPy's ability to apply a function to an entire row or column and broadcast when adding a scalar to a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cec89293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x_train, y_train, x_test, pdf=\"gaussian\", smoothing=1.0, eps=1e-9):\n",
    "    \"\"\"Predicts a Naive Bayes classifier\n",
    "       ---------------------------------\n",
    "       \n",
    "       Parameters:\n",
    "           x_train: input features for the training data set\n",
    "           y_train: class labels for the training data set\n",
    "           x_test: input features from the test data set to predict the labels with\n",
    "           pdf: the probability density fuction to use to calculate the likelihood; current options are guassian and multinomial\n",
    "           smoothing: a smoothing parameter for Laplace smoothing; if pdf is gaussian this prameter is ignored\n",
    "           eps: a constant to add to probabilities before taking their log to avoid dividing by zero\n",
    "           \n",
    "        Returns a tuple of predicted class labels and the log posteriors for each class and data point\"\"\"\n",
    "    \n",
    "    # Safety check\n",
    "    if pdf not in [\"multinomial\", \"gaussian\"]:\n",
    "        raise ValueError(\"This Naive Bayes implementation currently only supports multinomial and Gaussian distributions\")\n",
    "    \n",
    "    # Store the unique labels\n",
    "    labels = np.unique(y_train)\n",
    "    \n",
    "    # Get the priors\n",
    "    priors = fit(y_train)\n",
    "    \n",
    "    # Stores posteriors\n",
    "    posteriors_list = []\n",
    "    \n",
    "    # Stores predictions\n",
    "    predictions = []\n",
    "    \n",
    "    # Loop through each unique class\n",
    "    for label in labels:\n",
    "        \n",
    "        # Data subsetted to each label\n",
    "        subset = x_train[y_train == label]\n",
    "        \n",
    "        # Applies the log likelihood to every element in every row of the test data and calculates its posterior\n",
    "        if pdf == \"gaussian\":\n",
    "            posteriors = (priors[label] + np.apply_along_axis(gaussian_log_likelihood, 1, x_test, subset, eps)).tolist()\n",
    "        \n",
    "        if pdf == \"multinomial\":\n",
    "            posteriors = (priors[label] + np.apply_along_axis(multinomial_log_likelihood, 1, x_test, subset, \n",
    "                                                              smoothing, eps)).tolist()\n",
    "            \n",
    "        posteriors_list.append(posteriors)\n",
    "        \n",
    "    # Loop through the posteriors to find the class with the highest posterior\n",
    "    for idx in np.apply_along_axis(np.argmax, 1, np.array(posteriors_list).T):\n",
    "        predictions.append(int(labels[idx]))\n",
    "        \n",
    "    return np.array(predictions), np.array(posteriors_list).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01352874",
   "metadata": {},
   "source": [
    "### Evaluating the model\n",
    "No matter how complicated a model is, if it does not perform well, there is no sense in using it. Therfore, it would be useful to know some metrics like the number of true positives, true negatives, false positives, false negatives, accuracy, precision, recall, and F1 score, which the function below calculates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd06ee7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_actual, y_pred):\n",
    "    \"\"\"Calculates true positives, true negatives, false positives, false negatives, accuracy, precision, recall, and F1 score\n",
    "       ----------------------------------------------------------------------------------------------------------------------\n",
    "       \n",
    "       Parameters:\n",
    "            y_actual: actual labels\n",
    "            y_pred: predicted labels\n",
    "            \n",
    "       Returns a tuple of the true positives, true negatives, false positives, false negatives, accuracy, precision, recall, \n",
    "           and F1 score\"\"\"\n",
    "    \n",
    "    # Store the class labels\n",
    "    labels = np.unique(y_actual)\n",
    "    \n",
    "    # When there are only two class labels\n",
    "    if y_actual.size == 2:\n",
    "        \n",
    "        # Calculate positives and negatives\n",
    "        true_positive = np.sum(np.where((y_pred == 1) & (y_actual == 1)))\n",
    "        true_negative = np.sum(np.where((y_pred == 0) & (y_actual == 0)))\n",
    "        false_positive = np.sum(np.where((y_pred == 1) & (y_actual == 0)))\n",
    "        false_negative = np.sum(np.where((y_pred == 0) & (y_actual == 1)))\n",
    "    \n",
    "    # When doing multiclass classification\n",
    "    else:\n",
    "        \n",
    "        for label in labels:\n",
    "        \n",
    "            # Calculate positives and negatives\n",
    "            true_positive = np.sum(np.where((y_pred == label) & (y_actual == label)))\n",
    "            true_negative = np.sum(np.where((y_pred != label) & (y_actual != label)))\n",
    "            false_positive = np.sum(np.where((y_pred == label) & (y_actual != label)))\n",
    "            false_negative = np.sum(np.where((y_pred != label) & (y_actual == label)))\n",
    "            \n",
    "    # Calculate accuracy\n",
    "    accuracy = (true_positive + true_negative) / (true_positive + true_negative + false_positive + false_negative)\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    precision = true_positive / (true_positive + false_positive)\n",
    "    recall = true_positive / (true_positive + false_negative)\n",
    "    \n",
    "    # Calculate the F1 score\n",
    "    f1 = (2 * (precision * recall)) / (precision + recall)\n",
    "        \n",
    "    return true_positive, true_negative, false_positive, false_negative, accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1e78b7",
   "metadata": {},
   "source": [
    "### Using the model\n",
    "Now it is time to use the model. In the first cell, we assume a multinomial distribution to train, predict, ad evaluate our model. The second cell assumes a gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e98617e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1 score with a multiomial distribution is 0.8505722541329463\n"
     ]
    }
   ],
   "source": [
    "# Multinomial distribution\n",
    "\n",
    "# Train and predict\n",
    "class_predictions1, posterior_predictions1 = predict(x_train, y_train, x_test, pdf=\"multinomial\")\n",
    "\n",
    "# Evaluate\n",
    "evaluations1 = evaluate(y_test, class_predictions1)\n",
    "print(\"The F1 score with a multiomial distribution is \" + str(evaluations1[7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d246a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1 score with a Gaussian distribution is 0.31365711284270037\n"
     ]
    }
   ],
   "source": [
    "# Gaussian distribution\n",
    "\n",
    "# Train and predict\n",
    "class_predictions2, posterior_predictions2 = predict(x_train, y_train, x_test, pdf=\"gaussian\")\n",
    "\n",
    "# Evaluate\n",
    "evaluations2 = evaluate(y_test, class_predictions2)\n",
    "print(\"The F1 score with a Gaussian distribution is \" + str(evaluations2[7]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
